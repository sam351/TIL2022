
# Lecture 2: Word Vectors, Word Senses, and Neural Network Classifiers

## 1. Review: Main idea of word2vec
* Initialize random vectors for each word â†’ Predict surrounding words using center word, or vice versa
* Learning is done by Updating vectors so they can predict actual surrounding/center words better
<br><br><img src="images/lecture02_img1.png" height="100"> <br><br>
* Word2vec is "Bag of words" model - same prediction at each position (ignoring word positions in sentences)
* Word2vec maximizes objective function by putting similar words nearby in space

<br>

## 2. Optimization: Gradient Descent
### 1) 
* 

<br>

## 3. Why not capture co-occurrence counts directly?

<br>

## 4. Towards GloVe: Count based vs. direct prediction

<br>

## 5. How to evaluate word vectors?

<br>

## 6. Word senses and word sense ambiguity

<br>

## 7. Classification review and notation

<br>

## 8. Neural Network Classifiers



